{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa4d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "import scipy\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from scipy.stats import iqr\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import rankdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908b5f3",
   "metadata": {},
   "source": [
    "# Synthetization of 3,600 data cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8cd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range (1,10):\n",
    "    exec(f'd{n} = pd.read_csv(r\"C:\\\\__datacube{n}.csv\")')\n",
    "    exec(f'd{n}[\"Outlier\"]=0')\n",
    "    \n",
    "#8*3*30*5 = 3,600 data cubes   \n",
    "for n in range (2,10):\n",
    "    for p in [25, 100, 500]:  \n",
    "        for r in range (1,31):\n",
    "            for s in [10, 15, 20, 25, 30]:\n",
    "                random.seed(r)\n",
    "                #Splitting data in Noise and Outlier\n",
    "                exec(f'd{n}_o_{p}p_{r}rs = d{n}.sample(frac=({p}/10000), replace=False, random_state={r})')\n",
    "                exec(f'd{n}_n_{p}p_{r}rs = d{n}.drop(d{n}_o_{p}p_{r}rs.index)')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"Outlier\"]=1')\n",
    "\n",
    "                #Calculating IQR and mean on the non-outliers\n",
    "                for dim in [\"Time\", \"City\", \"Product\"]:\n",
    "                    exec(f'data_IQR = d{n}_n_{p}p_{r}rs[[dim, \"Sales\"]]')\n",
    "                    IQR = data_IQR.groupby(dim).apply(iqr)\n",
    "                    IQR = IQR.reset_index()\n",
    "                    IQR = IQR.rename(columns={0:\"IQR_\"+dim})\n",
    "                    IQR[\"IQR_\"+dim] = IQR[\"IQR_\"+dim].astype(float)\n",
    "                    exec(f'd{n}_o_{p}p_{r}rs = pd.merge(d{n}_o_{p}p_{r}rs, IQR, on = [dim], how = \"left\")')\n",
    "\n",
    "                for dim in [\"Time\", \"City\", \"Product\"]:\n",
    "                    exec(f'data_mean = d{n}_n_{p}p_{r}rs[[dim, \"Sales\"]]')\n",
    "                    Mean = data_mean.groupby(dim).apply(np.mean)\n",
    "                    Mean = Mean.reset_index()\n",
    "                    Mean = Mean.rename(columns={\"Sales\":\"Mean_\"+dim})\n",
    "                    exec(f'd{n}_o_{p}p_{r}rs = pd.merge(d{n}_o_{p}p_{r}rs, Mean, on = [dim], how = \"left\")')\n",
    "                \n",
    "                #Outlier = value nearest to genreated random value, that is an outlier in one dimension \n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"O_Time_p\"]= np.ceil(d{n}_o_{p}p_{r}rs[\"Mean_Time\"] + (2*d{n}_o_{p}p_{r}rs[\"IQR_Time\"]))')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"O_Time_n\"]= np.floor(d{n}_o_{p}p_{r}rs[\"Mean_Time\"] - (2*d{n}_o_{p}p_{r}rs[\"IQR_Time\"]))')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"O_City_p\"]= np.ceil(d{n}_o_{p}p_{r}rs[\"Mean_City\"] + (2*d{n}_o_{p}p_{r}rs[\"IQR_City\"]))')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"O_City_n\"]= np.floor(d{n}_o_{p}p_{r}rs[\"Mean_City\"] - (2*d{n}_o_{p}p_{r}rs[\"IQR_City\"]))')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"O_Product_p\"]= np.ceil(d{n}_o_{p}p_{r}rs[\"Mean_Product\"] + (2*d{n}_o_{p}p_{r}rs[\"IQR_Product\"]))')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"O_Product_n\"]= np.floor(d{n}_o_{p}p_{r}rs[\"Mean_Product\"] - (2*d{n}_o_{p}p_{r}rs[\"IQR_Product\"]))')\n",
    "                \n",
    "                \n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"Diff_Time_p\"]= np.absolute(d{n}_o_{p}p_{r}rs[\"Sales\"]-d{n}_o_{p}p_{r}rs[\"O_Time_p\"])')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"Diff_Time_n\"]= np.absolute(d{n}_o_{p}p_{r}rs[\"Sales\"]-d{n}_o_{p}p_{r}rs[\"O_Time_n\"])')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"Diff_City_p\"]= np.absolute(d{n}_o_{p}p_{r}rs[\"Sales\"]-d{n}_o_{p}p_{r}rs[\"O_City_p\"])')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"Diff_City_n\"]= np.absolute(d{n}_o_{p}p_{r}rs[\"Sales\"]-d{n}_o_{p}p_{r}rs[\"O_City_n\"])')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"Diff_Product_p\"]= np.absolute(d{n}_o_{p}p_{r}rs[\"Sales\"]-d{n}_o_{p}p_{r}rs[\"O_Product_p\"])')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[\"Diff_Product_n\"]= np.absolute(d{n}_o_{p}p_{r}rs[\"Sales\"]-d{n}_o_{p}p_{r}rs[\"O_Product_n\"])')\n",
    "                \n",
    "                def get_col(sr):\n",
    "                    name=np.abs(sr).idxmin()\n",
    "                    return pd.Series([name])\n",
    "                exec(f'd{n}_o_{p}p_{r}rs[[\"Min_col\"]] = d{n}_o_{p}p_{r}rs[[\"Diff_Time_p\", \"Diff_Time_n\", \"Diff_City_p\", \"Diff_City_n\", \"Diff_Product_p\", \"Diff_Product_n\"]].apply(lambda x : get_col(x), axis=1)')\n",
    "                exec(f'd{n}_o_{p}p_{r}rs.Min_col.replace([\"Diff_Time_p\", \"Diff_Time_n\", \"Diff_City_p\", \"Diff_City_n\", \"Diff_Product_p\", \"Diff_Product_n\"], [\"O_Time_p\", \"O_Time_n\", \"O_City_p\", \"O_City_n\", \"O_Product_p\", \"O_Product_n\"], inplace=True)')\n",
    "                \n",
    "                exec(f'len_df = len(d{n}_o_{p}p_{r}rs)')\n",
    "                for i in range(len_df):\n",
    "                    exec(f'd{n}_o_{p}p_{r}rs.loc[i, \"Sales\"] = d{n}_o_{p}p_{r}rs.loc[i, d{n}_o_{p}p_{r}rs.loc[i, \"Min_col\"]]')\n",
    "                \n",
    "                #Drop columns that were necessary for new Sales calculation\n",
    "                exec(f'd{n}_o_{p}p_{r}rs = d{n}_o_{p}p_{r}rs.iloc[:, :-19]')\n",
    "                \n",
    "                #Adding Noise and Outlier dataframe\n",
    "                exec(f'd{n}_{p}p_{r}rs = pd.concat([d{n}_o_{p}p_{r}rs, d{n}_n_{p}p_{r}rs])')\n",
    "                #exec(f'd{n}_{p}p_{r}rs.sort_index(inplace = True)')\n",
    "                \n",
    "                #Adding Noise to overall dataframe\n",
    "                exec(f'd{n}_{p}p_{r}rs_std = np.std(d{n}_{p}p_{r}rs[\"Sales\"])')\n",
    "                exec(f'd{n}_{p}p_{r}rs[\"Sales\"] = d{n}_{p}p_{r}rs[\"Sales\"] + (d{n}_{p}p_{r}rs_std/3*np.round(np.random.normal(0,{s}),0))')\n",
    "                \n",
    "                #Eliminating negative values\n",
    "                exec(f'd{n}_{p}p_{r}rs.loc[d{n}_{p}p_{r}rs[\"Sales\"] <0, \"Sales\"] =1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39274d5",
   "metadata": {},
   "source": [
    "# Key figures of the data cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed21da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for n in range (2,10):\n",
    "    for p in [25, 100, 500]:  \n",
    "        for r in range (1,31):\n",
    "            for s in [10, 15, 20, 25, 30]:\n",
    "                exec(f'Dataset = \"d{n}_{p}p_{r}rs\"')\n",
    "                Basisdataset = n\n",
    "                Percentage_Outliers = p/100\n",
    "                exec(f'Mean = d{n}_{p}p_{r}rs[\"Sales\"].mean()')\n",
    "                exec(f'Std = np.std(d{n}_{p}p_{r}rs[\"Sales\"])')\n",
    "                exec(f'Min = d{n}_{p}p_{r}rs[\"Sales\"].min()')\n",
    "                exec(f'Max = d{n}_{p}p_{r}rs[\"Sales\"].max()')\n",
    "                exec(f'Twentyfive = np.quantile(d{n}_{p}p_{r}rs[\"Sales\"], 0.25)')\n",
    "                exec(f'Seventyfive = np.quantile(d{n}_{p}p_{r}rs[\"Sales\"], 0.75)')\n",
    "                exec(f'Length = len(d{n}_{p}p_{r}rs)')\n",
    "                df.append((Dataset, Basisdataset, Percentage_Outliers, Mean, Std, Min, Max, Twentyfive, \n",
    "                           Seventyfive, Length))\n",
    "                cols=[\"Dataset\", \"Basisdataset\", \"Percentage_Outliers\", \"Mean\", \"Std\", \"Min\", \"Max\", \"Twentyfive\", \n",
    "                           \"Seventyfive\", \"Length\"]\n",
    "                summary = pd.DataFrame(df, columns=cols)\n",
    "                \n",
    "#summary.to_excel(r\"C:\\\\__Df_summary.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f2d9ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Basisdataset</th>\n",
       "      <th>Percentage_Outliers</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Twentyfive</th>\n",
       "      <th>Seventyfive</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>304.859730</td>\n",
       "      <td>31.783748</td>\n",
       "      <td>225.875246</td>\n",
       "      <td>397.489639</td>\n",
       "      <td>282.602238</td>\n",
       "      <td>326.268429</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>176.192907</td>\n",
       "      <td>14.084944</td>\n",
       "      <td>138.281968</td>\n",
       "      <td>217.921682</td>\n",
       "      <td>165.563517</td>\n",
       "      <td>185.378686</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>245.463408</td>\n",
       "      <td>25.237258</td>\n",
       "      <td>180.520880</td>\n",
       "      <td>321.051279</td>\n",
       "      <td>227.921770</td>\n",
       "      <td>262.436588</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>245.792338</td>\n",
       "      <td>25.324844</td>\n",
       "      <td>180.241343</td>\n",
       "      <td>326.296899</td>\n",
       "      <td>228.249790</td>\n",
       "      <td>262.955669</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>454.693433</td>\n",
       "      <td>42.001195</td>\n",
       "      <td>344.118347</td>\n",
       "      <td>598.374781</td>\n",
       "      <td>425.089439</td>\n",
       "      <td>482.508792</td>\n",
       "      <td>13000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>99.169131</td>\n",
       "      <td>7.544260</td>\n",
       "      <td>78.015224</td>\n",
       "      <td>122.581890</td>\n",
       "      <td>93.917457</td>\n",
       "      <td>104.410439</td>\n",
       "      <td>26000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>259.605273</td>\n",
       "      <td>19.532013</td>\n",
       "      <td>205.214834</td>\n",
       "      <td>318.814834</td>\n",
       "      <td>245.833961</td>\n",
       "      <td>273.014342</td>\n",
       "      <td>26000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>78.029411</td>\n",
       "      <td>7.660797</td>\n",
       "      <td>55.984554</td>\n",
       "      <td>102.796726</td>\n",
       "      <td>72.367382</td>\n",
       "      <td>83.189604</td>\n",
       "      <td>52000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Basisdataset  Percentage_Outliers        Mean        Std         Min  \\\n",
       "0             2             2.083333  304.859730  31.783748  225.875246   \n",
       "1             3             2.083333  176.192907  14.084944  138.281968   \n",
       "2             4             2.083333  245.463408  25.237258  180.520880   \n",
       "3             5             2.083333  245.792338  25.324844  180.241343   \n",
       "4             6             2.083333  454.693433  42.001195  344.118347   \n",
       "5             7             2.083333   99.169131   7.544260   78.015224   \n",
       "6             8             2.083333  259.605273  19.532013  205.214834   \n",
       "7             9             2.083333   78.029411   7.660797   55.984554   \n",
       "\n",
       "          Max  Twentyfive  Seventyfive   Length  \n",
       "0  397.489639  282.602238   326.268429   3000.0  \n",
       "1  217.921682  165.563517   185.378686   6000.0  \n",
       "2  321.051279  227.921770   262.436588   6000.0  \n",
       "3  326.296899  228.249790   262.955669  12000.0  \n",
       "4  598.374781  425.089439   482.508792  13000.0  \n",
       "5  122.581890   93.917457   104.410439  26000.0  \n",
       "6  318.814834  245.833961   273.014342  26000.0  \n",
       "7  102.796726   72.367382    83.189604  52000.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = summary.groupby(\"Basisdataset\").mean()\n",
    "datasets = datasets.reset_index()\n",
    "display(datasets)\n",
    "#datasets.to_excel(r\"C:\\\\__Basisdatasets.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7668a4e5",
   "metadata": {},
   "source": [
    "# RODD S-75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d0ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "ds = []\n",
    "for n in range (2,10):\n",
    "    for p in [25,100,500]: \n",
    "        for r in range (1,31):\n",
    "            for s in [10, 15, 20, 25, 30]:\n",
    "                num_v = \"Sales\"\n",
    "                cat_dims = [\"Time\", \"City\", \"Product\"]\n",
    "                exec(f'data_log = d{n}_{p}p_{r}rs.copy()')\n",
    "                data_log[num_v] = np.log(data_log[num_v])\n",
    "                #Calculating the trimmed means\n",
    "                mean_num_v = stats.trim_mean(data_log[num_v], 0.125, axis=0)\n",
    "                for dim in cat_dims:\n",
    "                    data_means = data_log[[dim, num_v]]\n",
    "                    trimmed_means = data_means.groupby(dim).apply(stats.trim_mean, 0.125)\n",
    "                    trimmed_means = trimmed_means.reset_index()\n",
    "                    trimmed_means = trimmed_means.rename(columns={0:\"trimmed_mean_\"+dim})\n",
    "                    trimmed_means[\"trimmed_mean_\"+dim] = trimmed_means[\"trimmed_mean_\"+dim].astype(float)\n",
    "                    data_log = pd.merge(data_log, trimmed_means, on = [dim], how = 'left')\n",
    "                for dim1 in cat_dims:\n",
    "                    for dim2 in cat_dims:\n",
    "                        if dim1 != dim2: \n",
    "                            data_means = data_log[[dim1, dim2, num_v]]\n",
    "                            trimmed_means = data_means.groupby([dim1, dim2]).apply(stats.trim_mean, 0.125)\n",
    "                            trimmed_means = trimmed_means.reset_index()\n",
    "                            trimmed_means = trimmed_means.rename(columns={0:\"trimmed_mean_\"+dim1+\"_\"+dim2})\n",
    "                            trimmed_means[\"trimmed_mean_\"+dim1+\"_\"+dim2] = trimmed_means[\"trimmed_mean_\"+dim1+\"_\"+dim2].astype(float)\n",
    "                            data_log = pd.merge(data_log, trimmed_means, on = [dim1, dim2], how = 'left')\n",
    "                #Deleting duplicate combinations (it does not matter whether you combine A to B or B to A)\n",
    "                data_log = data_log.iloc[:, :-2]\n",
    "                data_log = data_log.drop(data_log.iloc[:,[-2]], axis='columns')\n",
    "                #Calculating the expected value (based on a truncated version of the formula)\n",
    "                data_log[\"expected_value\"] = np.exp((\n",
    "                                                    #trimmed mean AB\n",
    "                                                    data_log[data_log.columns[-1]] \n",
    "                                                    #trimmed mean AC\n",
    "                                                    + data_log[data_log.columns[-2]]\n",
    "                                                    #trimmed mean AB\n",
    "                                                    +data_log[data_log.columns[-3]] \n",
    "                                                    #trimmed mean C \n",
    "                                                    - data_log[data_log.columns[-4]]\n",
    "                                                    #trimmed mean B \n",
    "                                                    -data_log[data_log.columns[-5]] \n",
    "                                                    #trimmed mean A\n",
    "                                                    - data_log[data_log.columns[-6]]\n",
    "                                                    #trimmed mean of all cells\n",
    "                                                    + mean_num_v))\n",
    "\n",
    "                data_log[\"Sales_value\"] = np.exp(data_log[num_v])\n",
    "                #Auxiliary variable for calculating the variance\n",
    "                data_log[\"X1\"] = pow((data_log[\"Sales_value\"]-data_log[\"expected_value\"]),2)*np.log(data_log[\"expected_value\"])    \n",
    "                x0 = [1]\n",
    "                #Calculation of the variance\n",
    "                def fun(x):\n",
    "                    return (data_log[\"X1\"]/pow(data_log[\"expected_value\"],x)).sum()-np.log(data_log[\"expected_value\"]).sum()\n",
    "                res = scipy.optimize.fsolve(fun, 1)\n",
    "                variance = pow(data_log[\"expected_value\"].mean(), res)\n",
    "                standard_deviation = pow(variance, 0.5)\n",
    "\n",
    "                #Calculation of the SelfExp value\n",
    "                data_log[\"SelfExp\"]=abs((data_log[\"expected_value\"]-data_log[\"Sales_value\"]))/standard_deviation\n",
    "                data_log[\"S_Outlier\"] = np.where(data_log['SelfExp']>= 2.5, True, False)\n",
    "\n",
    "                #Performance Evaluation\n",
    "                Total_Run_Time = time() - start\n",
    "                exec(f'Dataset = \"d{n}_{p}p_{r}rs\"')\n",
    "                Basis_Dataset = n\n",
    "                Length_of_Dataset = len(data_log)\n",
    "                Std_of_Noise = s\n",
    "                True_percentage_of_Outliers = p\n",
    "                TP = len(data_log[(data_log[\"S_Outlier\"]==True) & (data_log[\"Outlier\"]==1)])\n",
    "                FP = len(data_log[(data_log[\"S_Outlier\"]==True) & (data_log[\"Outlier\"]==0)])\n",
    "                TN = len(data_log[(data_log[\"S_Outlier\"]==False) & (data_log[\"Outlier\"]==0)])\n",
    "                FN = len(data_log[(data_log[\"S_Outlier\"]==False) & (data_log[\"Outlier\"]==1)])\n",
    "                \n",
    "                Sensitivity = TP / (TP + FN)\n",
    "                Specificity = TN / (TN + FP)\n",
    "                Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "                AUC_score = roc_auc_score(data_log[\"Outlier\"], data_log[\"SelfExp\"])\n",
    "\n",
    "                ds.append((Total_Run_Time, Dataset, Basis_Dataset, Length_of_Dataset, Std_of_Noise, \n",
    "                           True_percentage_of_Outliers, TP, FP, TN, FN, Sensitivity, Specificity, Accuracy, AUC_score))\n",
    "\n",
    "                cols=[\"Total_Run_Time\", \"Dataset\", \"Basis_Dataset\", 'Length_of_Dataset', \"Std_of_Noise\", \n",
    "                      \"True_percentage_of_Outliers\", \"TP\", \"FP\", \"TN\", \"FN\", \"Sensitivity\", \"Specificity\", \"Accuracy\",\n",
    "                     \"AUC_score\"]\n",
    "\n",
    "                result = pd.DataFrame(ds, columns=cols)\n",
    "result[\"Total_Run_Time_shift\"] = result.Total_Run_Time.shift(1) \n",
    "result[\"Total_Run_Time_shift\"][0] = 0\n",
    "result[\"Run_Time\"] = result[\"Total_Run_Time\"] - result[\"Total_Run_Time_shift\"]\n",
    "result = result.drop([\"Total_Run_Time\", \"Total_Run_Time_shift\"], axis=1)\n",
    "\n",
    "evaluation = result[[\"Basis_Dataset\", \"Sensitivity\", \"Specificity\", \"Accuracy\", \"AUC_score\"]]\n",
    "evaluation = evaluation.groupby(\"Basis_Dataset\").mean()\n",
    "evaluation = evaluation.reset_index()\n",
    "#evaluation.to_excel(r\"C:\\\\__SelfExp_evaluation.xlsx\")\n",
    "#result.to_excel(r\"C:\\\\__SelfExp.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39667397",
   "metadata": {},
   "source": [
    "# RODD - S60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "ds = []\n",
    "for n in range (2,10):\n",
    "    for p in [25,100,500]: \n",
    "        for r in range (1,31):\n",
    "            for s in [10, 15, 20, 25, 30]:\n",
    "                num_v = \"Sales\"\n",
    "                cat_dims = [\"Time\", \"City\", \"Product\"]\n",
    "                exec(f'data_log = d{n}_{p}p_{r}rs.copy()')\n",
    "                data_log[num_v] = np.log(data_log[num_v])\n",
    "                #Calculating the trimmed means\n",
    "                mean_num_v = stats.trim_mean(data_log[num_v], 0.2, axis=0)\n",
    "                for dim in cat_dims:\n",
    "                    data_means = data_log[[dim, num_v]]\n",
    "                    trimmed_means = data_means.groupby(dim).apply(stats.trim_mean, 0.2)\n",
    "                    trimmed_means = trimmed_means.reset_index()\n",
    "                    trimmed_means = trimmed_means.rename(columns={0:\"trimmed_mean_\"+dim})\n",
    "                    trimmed_means[\"trimmed_mean_\"+dim] = trimmed_means[\"trimmed_mean_\"+dim].astype(float)\n",
    "                    data_log = pd.merge(data_log, trimmed_means, on = [dim], how = 'left')\n",
    "                for dim1 in cat_dims:\n",
    "                    for dim2 in cat_dims:\n",
    "                        if dim1 != dim2: \n",
    "                            data_means = data_log[[dim1, dim2, num_v]]\n",
    "                            trimmed_means = data_means.groupby([dim1, dim2]).apply(stats.trim_mean, 0.2)\n",
    "                            trimmed_means = trimmed_means.reset_index()\n",
    "                            trimmed_means = trimmed_means.rename(columns={0:\"trimmed_mean_\"+dim1+\"_\"+dim2})\n",
    "                            trimmed_means[\"trimmed_mean_\"+dim1+\"_\"+dim2] = trimmed_means[\"trimmed_mean_\"+dim1+\"_\"+dim2].astype(float)\n",
    "                            data_log = pd.merge(data_log, trimmed_means, on = [dim1, dim2], how = 'left')\n",
    "                #Deleting duplicate combinations (it does not matter whether you combine A to B or B to A)\n",
    "                data_log = data_log.iloc[:, :-2]\n",
    "                data_log = data_log.drop(data_log.iloc[:,[-2]], axis='columns')\n",
    "                #Calculating the expected value (based on a truncated version of the formula)\n",
    "                data_log[\"expected_value\"] = np.exp((\n",
    "                                                    #trimmed mean AB\n",
    "                                                    data_log[data_log.columns[-1]] \n",
    "                                                    #trimmed mean AC\n",
    "                                                    + data_log[data_log.columns[-2]]\n",
    "                                                    #trimmed mean AB\n",
    "                                                    +data_log[data_log.columns[-3]] \n",
    "                                                    #trimmed mean C \n",
    "                                                    - data_log[data_log.columns[-4]]\n",
    "                                                    #trimmed mean B \n",
    "                                                    -data_log[data_log.columns[-5]] \n",
    "                                                    #trimmed mean A\n",
    "                                                    - data_log[data_log.columns[-6]]\n",
    "                                                    #trimmed mean of all cells\n",
    "                                                    + mean_num_v))\n",
    "\n",
    "                data_log[\"Sales_value\"] = np.exp(data_log[num_v])\n",
    "                #Auxiliary variable for calculating the variance\n",
    "                data_log[\"X1\"] = pow((data_log[\"Sales_value\"]-data_log[\"expected_value\"]),2)*np.log(data_log[\"expected_value\"])    \n",
    "                x0 = [1]\n",
    "                #Calculation of the variance\n",
    "                def fun(x):\n",
    "                    return (data_log[\"X1\"]/pow(data_log[\"expected_value\"],x)).sum()-np.log(data_log[\"expected_value\"]).sum()\n",
    "                res = scipy.optimize.fsolve(fun, 1)\n",
    "                variance = pow(data_log[\"expected_value\"].mean(), res)\n",
    "                standard_deviation = pow(variance, 0.5)\n",
    "\n",
    "                #Calculation of the SelfExp value\n",
    "                data_log[\"SelfExp\"]=abs((data_log[\"expected_value\"]-data_log[\"Sales_value\"]))/standard_deviation\n",
    "                data_log[\"S_Outlier\"] = np.where(data_log['SelfExp']>= 2.5, True, False)\n",
    "\n",
    "                #Performance Evaluation\n",
    "                Total_Run_Time = time() - start\n",
    "                exec(f'Dataset = \"d{n}_{p}p_{r}rs\"')\n",
    "                Basis_Dataset = n\n",
    "                Length_of_Dataset = len(data_log)\n",
    "                Std_of_Noise = s\n",
    "                True_percentage_of_Outliers = p\n",
    "                TP = len(data_log[(data_log[\"S_Outlier\"]==True) & (data_log[\"Outlier\"]==1)])\n",
    "                FP = len(data_log[(data_log[\"S_Outlier\"]==True) & (data_log[\"Outlier\"]==0)])\n",
    "                TN = len(data_log[(data_log[\"S_Outlier\"]==False) & (data_log[\"Outlier\"]==0)])\n",
    "                FN = len(data_log[(data_log[\"S_Outlier\"]==False) & (data_log[\"Outlier\"]==1)])\n",
    "                \n",
    "                Sensitivity = TP / (TP + FN)\n",
    "                Specificity = TN / (TN + FP)\n",
    "                Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "                AUC_score = roc_auc_score(data_log[\"Outlier\"], data_log[\"SelfExp\"])\n",
    "\n",
    "                ds.append((Total_Run_Time, Dataset, Basis_Dataset, Length_of_Dataset, Std_of_Noise, \n",
    "                           True_percentage_of_Outliers, TP, FP, TN, FN, Sensitivity, Specificity, Accuracy, AUC_score))\n",
    "\n",
    "                cols=[\"Total_Run_Time\", \"Dataset\", \"Basis_Dataset\", 'Length_of_Dataset', \"Std_of_Noise\", \n",
    "                      \"True_percentage_of_Outliers\", \"TP\", \"FP\", \"TN\", \"FN\", \"Sensitivity\", \"Specificity\", \"Accuracy\",\n",
    "                     \"AUC_score\"]\n",
    "\n",
    "                result = pd.DataFrame(ds, columns=cols)\n",
    "result[\"Total_Run_Time_shift\"] = result.Total_Run_Time.shift(1) \n",
    "result[\"Total_Run_Time_shift\"][0] = 0\n",
    "result[\"Run_Time\"] = result[\"Total_Run_Time\"] - result[\"Total_Run_Time_shift\"]\n",
    "result = result.drop([\"Total_Run_Time\", \"Total_Run_Time_shift\"], axis=1)\n",
    "\n",
    "evaluation = result[[\"Basis_Dataset\", \"Sensitivity\", \"Specificity\", \"Accuracy\", \"AUC_score\"]]\n",
    "evaluation = evaluation.groupby(\"Basis_Dataset\").mean()\n",
    "evaluation = evaluation.reset_index()\n",
    "#evaluation.to_excel(r\"C:\\\\__SelfExp_60_evaluation.xlsx\")\n",
    "#result.to_excel(r\"C:\\\\__SelfExp_60.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec53a20",
   "metadata": {},
   "source": [
    "# RODD - S90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "ds = []\n",
    "for n in range (2,10):\n",
    "    for p in [25,100,500]: \n",
    "        for r in range (1,31):\n",
    "            for s in [10, 15, 20, 25, 30]:\n",
    "                num_v = \"Sales\"\n",
    "                cat_dims = [\"Time\", \"City\", \"Product\"]\n",
    "                exec(f'data_log = d{n}_{p}p_{r}rs.copy()')\n",
    "                data_log[num_v] = np.log(data_log[num_v])\n",
    "                #Calculating the trimmed means\n",
    "                mean_num_v = stats.trim_mean(data_log[num_v], 0.05, axis=0)\n",
    "                for dim in cat_dims:\n",
    "                    data_means = data_log[[dim, num_v]]\n",
    "                    trimmed_means = data_means.groupby(dim).apply(stats.trim_mean, 0.05)\n",
    "                    trimmed_means = trimmed_means.reset_index()\n",
    "                    trimmed_means = trimmed_means.rename(columns={0:\"trimmed_mean_\"+dim})\n",
    "                    trimmed_means[\"trimmed_mean_\"+dim] = trimmed_means[\"trimmed_mean_\"+dim].astype(float)\n",
    "                    data_log = pd.merge(data_log, trimmed_means, on = [dim], how = 'left')\n",
    "                for dim1 in cat_dims:\n",
    "                    for dim2 in cat_dims:\n",
    "                        if dim1 != dim2: \n",
    "                            data_means = data_log[[dim1, dim2, num_v]]\n",
    "                            trimmed_means = data_means.groupby([dim1, dim2]).apply(stats.trim_mean, 0.05)\n",
    "                            trimmed_means = trimmed_means.reset_index()\n",
    "                            trimmed_means = trimmed_means.rename(columns={0:\"trimmed_mean_\"+dim1+\"_\"+dim2})\n",
    "                            trimmed_means[\"trimmed_mean_\"+dim1+\"_\"+dim2] = trimmed_means[\"trimmed_mean_\"+dim1+\"_\"+dim2].astype(float)\n",
    "                            data_log = pd.merge(data_log, trimmed_means, on = [dim1, dim2], how = 'left')\n",
    "                #Deleting duplicate combinations (it does not matter whether you combine A to B or B to A)\n",
    "                data_log = data_log.iloc[:, :-2]\n",
    "                data_log = data_log.drop(data_log.iloc[:,[-2]], axis='columns')\n",
    "                #Calculating the expected value (based on a truncated version of the formula)\n",
    "                data_log[\"expected_value\"] = np.exp((\n",
    "                                                    #trimmed mean AB\n",
    "                                                    data_log[data_log.columns[-1]] \n",
    "                                                    #trimmed mean AC\n",
    "                                                    + data_log[data_log.columns[-2]]\n",
    "                                                    #trimmed mean AB\n",
    "                                                    +data_log[data_log.columns[-3]] \n",
    "                                                    #trimmed mean C \n",
    "                                                    - data_log[data_log.columns[-4]]\n",
    "                                                    #trimmed mean B \n",
    "                                                    -data_log[data_log.columns[-5]] \n",
    "                                                    #trimmed mean A\n",
    "                                                    - data_log[data_log.columns[-6]]\n",
    "                                                    #trimmed mean of all cells\n",
    "                                                    + mean_num_v))\n",
    "\n",
    "                data_log[\"Sales_value\"] = np.exp(data_log[num_v])\n",
    "                #Auxiliary variable for calculating the variance\n",
    "                data_log[\"X1\"] = pow((data_log[\"Sales_value\"]-data_log[\"expected_value\"]),2)*np.log(data_log[\"expected_value\"])    \n",
    "                x0 = [1]\n",
    "                #Calculation of the variance\n",
    "                def fun(x):\n",
    "                    return (data_log[\"X1\"]/pow(data_log[\"expected_value\"],x)).sum()-np.log(data_log[\"expected_value\"]).sum()\n",
    "                res = scipy.optimize.fsolve(fun, 1)\n",
    "                variance = pow(data_log[\"expected_value\"].mean(), res)\n",
    "                standard_deviation = pow(variance, 0.5)\n",
    "\n",
    "                #Calculation of the SelfExp value\n",
    "                data_log[\"SelfExp\"]=abs((data_log[\"expected_value\"]-data_log[\"Sales_value\"]))/standard_deviation\n",
    "                data_log[\"S_Outlier\"] = np.where(data_log['SelfExp']>= 2.5, True, False)\n",
    "\n",
    "                #Performance Evaluation\n",
    "                Total_Run_Time = time() - start\n",
    "                exec(f'Dataset = \"d{n}_{p}p_{r}rs\"')\n",
    "                Basis_Dataset = n\n",
    "                Length_of_Dataset = len(data_log)\n",
    "                Std_of_Noise = s\n",
    "                True_percentage_of_Outliers = p\n",
    "                TP = len(data_log[(data_log[\"S_Outlier\"]==True) & (data_log[\"Outlier\"]==1)])\n",
    "                FP = len(data_log[(data_log[\"S_Outlier\"]==True) & (data_log[\"Outlier\"]==0)])\n",
    "                TN = len(data_log[(data_log[\"S_Outlier\"]==False) & (data_log[\"Outlier\"]==0)])\n",
    "                FN = len(data_log[(data_log[\"S_Outlier\"]==False) & (data_log[\"Outlier\"]==1)])\n",
    "                \n",
    "                Sensitivity = TP / (TP + FN)\n",
    "                Specificity = TN / (TN + FP)\n",
    "                Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "                AUC_score = roc_auc_score(data_log[\"Outlier\"], data_log[\"SelfExp\"])\n",
    "\n",
    "                ds.append((Total_Run_Time, Dataset, Basis_Dataset, Length_of_Dataset, Std_of_Noise, \n",
    "                           True_percentage_of_Outliers, TP, FP, TN, FN, Sensitivity, Specificity, Accuracy, AUC_score))\n",
    "\n",
    "                cols=[\"Total_Run_Time\", \"Dataset\", \"Basis_Dataset\", 'Length_of_Dataset', \"Std_of_Noise\", \n",
    "                      \"True_percentage_of_Outliers\", \"TP\", \"FP\", \"TN\", \"FN\", \"Sensitivity\", \"Specificity\", \"Accuracy\",\n",
    "                     \"AUC_score\"]\n",
    "\n",
    "                result = pd.DataFrame(ds, columns=cols)\n",
    "result[\"Total_Run_Time_shift\"] = result.Total_Run_Time.shift(1) \n",
    "result[\"Total_Run_Time_shift\"][0] = 0\n",
    "result[\"Run_Time\"] = result[\"Total_Run_Time\"] - result[\"Total_Run_Time_shift\"]\n",
    "result = result.drop([\"Total_Run_Time\", \"Total_Run_Time_shift\"], axis=1)\n",
    "\n",
    "evaluation = result[[\"Basis_Dataset\", \"Sensitivity\", \"Specificity\", \"Accuracy\", \"AUC_score\"]]\n",
    "evaluation = evaluation.groupby(\"Basis_Dataset\").mean()\n",
    "evaluation = evaluation.reset_index()\n",
    "#evaluation.to_excel(r\"C:\\\\__SelfExp_90_evaluation.xlsx\")\n",
    "#result.to_excel(r\"C:\\\\__SelfExp_90.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e10669",
   "metadata": {},
   "source": [
    "# RODD - Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa38a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "ds = []\n",
    "for n in range (2,10):\n",
    "    for p in [25,100,500]: \n",
    "        for r in range (1,31):\n",
    "            for s in [10, 15, 20, 25, 30]:\n",
    "                num_v = \"Sales\"\n",
    "                cat_dims = [\"Time\", \"City\", \"Product\"]\n",
    "                exec(f'data_log = d{n}_{p}p_{r}rs.copy()')\n",
    "                data_log[num_v] = scipy.stats.rankdata(data_log[num_v])\n",
    "                data_log[num_v] = np.log(data_log[num_v])\n",
    "                #Calculating the trimmed means\n",
    "                mean_num_v = np.median(data_log[num_v])\n",
    "                for dim in cat_dims:\n",
    "                    data_medians = data_log[[dim, num_v]]\n",
    "                    medians = data_medians.groupby(dim).apply(np.median)\n",
    "                    medians = medians.reset_index()\n",
    "                    medians = medians.rename(columns={0:\"median_\"+dim})\n",
    "                    medians[\"median_\"+dim] = medians[\"median_\"+dim].astype(float)\n",
    "                    data_log = pd.merge(data_log, medians, on = [dim], how = 'left')\n",
    "                for dim1 in cat_dims:\n",
    "                    for dim2 in cat_dims:\n",
    "                        if dim1 != dim2: \n",
    "                            data_medians = data_log[[dim1, dim2, num_v]]\n",
    "                            medians = data_medians.groupby([dim1, dim2]).apply(np.median)\n",
    "                            medians = medians.reset_index()\n",
    "                            medians = medians.rename(columns={0:\"median_\"+dim1+\"_\"+dim2})\n",
    "                            medians[\"median_\"+dim1+\"_\"+dim2] = medians[\"median_\"+dim1+\"_\"+dim2].astype(float)\n",
    "                            data_log = pd.merge(data_log, medians, on = [dim1, dim2], how = 'left')\n",
    "                #Deleting duplicate combinations (it does not matter whether you combine A to B or B to A)\n",
    "                data_log = data_log.iloc[:, :-2]\n",
    "                data_log = data_log.drop(data_log.iloc[:,[-2]], axis='columns')\n",
    "                #Calculating the expected value (based on a truncated version of the formula)\n",
    "                data_log[\"expected_value\"] = np.exp((\n",
    "                                                    #trimmed mean AB\n",
    "                                                    data_log[data_log.columns[-1]] \n",
    "                                                    #trimmed mean AC\n",
    "                                                    + data_log[data_log.columns[-2]]\n",
    "                                                    #trimmed mean AB\n",
    "                                                    +data_log[data_log.columns[-3]] \n",
    "                                                    #trimmed mean C \n",
    "                                                    - data_log[data_log.columns[-4]]\n",
    "                                                    #trimmed mean B \n",
    "                                                    -data_log[data_log.columns[-5]] \n",
    "                                                    #trimmed mean A\n",
    "                                                    - data_log[data_log.columns[-6]]\n",
    "                                                    #trimmed mean of all cells\n",
    "                                                    + mean_num_v))\n",
    "\n",
    "                data_log[\"Sales_value\"] = np.exp(data_log[num_v])\n",
    "                #Auxiliary variable for calculating the variance\n",
    "                data_log[\"X1\"] = pow((data_log[\"Sales_value\"]-data_log[\"expected_value\"]),2)*np.log(data_log[\"expected_value\"])    \n",
    "                x0 = [1]\n",
    "                #Calculation of the variance\n",
    "                def fun(x):\n",
    "                    return (data_log[\"X1\"]/pow(data_log[\"expected_value\"],x)).sum()-np.log(data_log[\"expected_value\"]).sum()\n",
    "                res = scipy.optimize.fsolve(fun, 1)\n",
    "                variance = pow(data_log[\"expected_value\"].mean(), res)\n",
    "                standard_deviation = pow(variance, 0.5)\n",
    "\n",
    "                #Calculation of the SelfExp value\n",
    "                data_log[\"SelfExp\"]=abs((data_log[\"expected_value\"]-data_log[\"Sales_value\"]))/standard_deviation\n",
    "                data_log[\"S_Outlier\"] = np.where(data_log['SelfExp']>= 2.5, True, False)\n",
    "\n",
    "                #Performance Evaluation\n",
    "                Total_Run_Time = time() - start\n",
    "                exec(f'Dataset = \"d{n}_{p}p_{r}rs\"')\n",
    "                Basis_Dataset = n\n",
    "                Length_of_Dataset = len(data_log)\n",
    "                Std_of_Noise = s\n",
    "                True_percentage_of_Outliers = p\n",
    "                TP = len(data_log[(data_log[\"S_Outlier\"]==True) & (data_log[\"Outlier\"]==1)])\n",
    "                FP = len(data_log[(data_log[\"S_Outlier\"]==True) & (data_log[\"Outlier\"]==0)])\n",
    "                TN = len(data_log[(data_log[\"S_Outlier\"]==False) & (data_log[\"Outlier\"]==0)])\n",
    "                FN = len(data_log[(data_log[\"S_Outlier\"]==False) & (data_log[\"Outlier\"]==1)])\n",
    "                \n",
    "                Sensitivity = TP / (TP + FN)\n",
    "                Specificity = TN / (TN + FP)\n",
    "                Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "                AUC_score = roc_auc_score(data_log[\"Outlier\"], data_log[\"SelfExp\"])\n",
    "                \n",
    "                ds.append((Total_Run_Time, Dataset, Basis_Dataset, Length_of_Dataset, Std_of_Noise, \n",
    "                           True_percentage_of_Outliers, TP, FP, TN, FN, Sensitivity, Specificity, Accuracy, AUC_score))\n",
    "\n",
    "                cols=[\"Total_Run_Time\", \"Dataset\", \"Basis_Dataset\", 'Length_of_Dataset', \"Std_of_Noise\", \n",
    "                      \"True_percentage_of_Outliers\", \"TP\", \"FP\", \"TN\", \"FN\", \"Sensitivity\", \"Specificity\", \"Accuracy\",\n",
    "                     \"AUC_score\"]\n",
    "\n",
    "                result = pd.DataFrame(ds, columns=cols)\n",
    "result[\"Total_Run_Time_shift\"] = result.Total_Run_Time.shift(1) \n",
    "result[\"Total_Run_Time_shift\"][0] = 0\n",
    "result[\"Run_Time\"] = result[\"Total_Run_Time\"] - result[\"Total_Run_Time_shift\"]\n",
    "result = result.drop([\"Total_Run_Time\", \"Total_Run_Time_shift\"], axis=1)\n",
    "\n",
    "evaluation = result[[\"Basis_Dataset\", \"Sensitivity\", \"Specificity\", \"Accuracy\", \"AUC_score\"]]\n",
    "evaluation = evaluation.groupby(\"Basis_Dataset\").mean()\n",
    "evaluation = evaluation.reset_index()\n",
    "#evaluation.to_excel(r\"C:\\\\__Median_evaluation.xlsx\")\n",
    "#result.to_excel(r\"C:\\\\__Median.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee86ac",
   "metadata": {},
   "source": [
    "# RODD - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4611b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "ds = []\n",
    "for n in range (2,10):\n",
    "    for p in [25,100,500]: \n",
    "        for r in range (1,31):\n",
    "            for s in [10, 15, 20, 25, 30]:\n",
    "                exec(f'data_log = d{n}_{p}p_{r}rs.copy()')\n",
    "                num_v = \"Sales\"\n",
    "\n",
    "                #Training a RF\n",
    "                features = data_log[[\"Time\", \"City\", \"Product\", \"Outlier\"]]\n",
    "                train_features, test_features, train_labels, test_labels = train_test_split(features, data_log[num_v], test_size = 0.1, random_state = 42)\n",
    "                train_features_en = train_features.drop(\"Outlier\", axis = 1)\n",
    "                train_features_en = pd.get_dummies(train_features_en)\n",
    "                test_features_en = test_features.drop(\"Outlier\", axis = 1)\n",
    "                test_features_en = pd.get_dummies(test_features_en)\n",
    "                rf = RandomForestRegressor(n_estimators = 20, random_state = 42)\n",
    "                rf.fit(train_features_en, train_labels)                \n",
    "                predictions_train = rf.predict(train_features_en)\n",
    "                predictions_test = rf.predict(test_features_en)\n",
    "                train_features[\"expected_value\"] = predictions_train\n",
    "                test_features[\"expected_value\"] = predictions_test\n",
    "                \n",
    "                predictions = train_features.append(test_features)\n",
    "                data_log = pd.merge(data_log, predictions, on = [\"Time\", \"City\", \"Product\", \"Outlier\"], how = 'left') \n",
    "\n",
    "                #Auxiliary variable for calculating the variance\n",
    "                data_log[\"X1\"] = pow((data_log[num_v]-data_log[\"expected_value\"]),2)*np.log(data_log[\"expected_value\"])    \n",
    "                x0 = [1]\n",
    "                #Calculation of the variance\n",
    "                def fun(x):\n",
    "                    return (data_log[\"X1\"]/pow(data_log[\"expected_value\"],x)).sum()-np.log(data_log[\"expected_value\"]).sum()\n",
    "                res = scipy.optimize.fsolve(fun, 1)\n",
    "                variance = pow(data_log[\"expected_value\"].mean(), res)\n",
    "                standard_deviation = pow(variance, 0.5)\n",
    "\n",
    "                #Calculation of the SelfExp value\n",
    "                data_log[\"SelfExp\"]=abs((data_log[\"expected_value\"]-data_log[num_v]))/standard_deviation\n",
    "                data_log[\"S_Outlier\"] = np.where(data_log['SelfExp']>= 2.5, True, False)\n",
    "\n",
    "                #Performance Evaluation\n",
    "                Total_Run_Time = time() - start\n",
    "                exec(f'Dataset = \"d{n}_{p}p_{r}rs\"')\n",
    "                Basis_Dataset = n\n",
    "                Length_of_Dataset = len(data_log)\n",
    "                Std_of_Noise = s\n",
    "                True_percentage_of_Outliers = p\n",
    "                TP = len(data_log[(data_log[\"S_Outlier\"]==True) & (data_log[\"Outlier\"]==1)])\n",
    "                FP = len(data_log[(data_log[\"S_Outlier\"]==True) & (data_log[\"Outlier\"]==0)])\n",
    "                TN = len(data_log[(data_log[\"S_Outlier\"]==False) & (data_log[\"Outlier\"]==0)])\n",
    "                FN = len(data_log[(data_log[\"S_Outlier\"]==False) & (data_log[\"Outlier\"]==1)])\n",
    "                \n",
    "                Sensitivity = TP / (TP + FN)\n",
    "                Specificity = TN / (TN + FP)\n",
    "                Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "                AUC_score = roc_auc_score(data_log[\"Outlier\"], data_log[\"SelfExp\"])\n",
    "\n",
    "                ds.append((Total_Run_Time, Dataset, Basis_Dataset, Length_of_Dataset, Std_of_Noise, \n",
    "                           True_percentage_of_Outliers, TP, FP, TN, FN, Sensitivity, Specificity, Accuracy, AUC_score))\n",
    "\n",
    "                cols=[\"Total_Run_Time\", \"Dataset\", \"Basis_Dataset\", 'Length_of_Dataset', \"Std_of_Noise\", \n",
    "                      \"True_percentage_of_Outliers\", \"TP\", \"FP\", \"TN\", \"FN\", \"Sensitivity\", \"Specificity\", \"Accuracy\",\n",
    "                     \"AUC_score\"]\n",
    "\n",
    "                result = pd.DataFrame(ds, columns=cols)\n",
    "result[\"Total_Run_Time_shift\"] = result.Total_Run_Time.shift(1) \n",
    "result[\"Total_Run_Time_shift\"][0] = 0\n",
    "result[\"Run_Time\"] = result[\"Total_Run_Time\"] - result[\"Total_Run_Time_shift\"]\n",
    "result = result.drop([\"Total_Run_Time\", \"Total_Run_Time_shift\"], axis=1)\n",
    "\n",
    "evaluation = result[[\"Basis_Dataset\", \"Sensitivity\", \"Specificity\", \"Accuracy\", \"AUC_score\"]]\n",
    "evaluation = evaluation.groupby(\"Basis_Dataset\").mean()\n",
    "evaluation = evaluation.reset_index()\n",
    "#evaluation.to_excel(r\"C:\\\\__RF_evaluation.xlsx\")\n",
    "#result.to_excel(r\"C:\\\\__RF.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
